# ELT Pipeline with Apache Airflow and dbt

This project demonstrates a complete **Extract-Load-Transform (ELT)** data pipeline using **Apache Airflow**, **Docker**, and **dbt**. It loads data from a source PostgreSQL database, transforms it with `dbt`, and stores results in a destination PostgreSQL database.

## ğŸ› ï¸ Project Components

- **Apache Airflow**: Workflow orchestrator to manage ELT steps.
- **dbt (Data Build Tool)**: SQL-based transformations on the destination database.
- **PostgreSQL (source & destination)**: Simulated databases for data movement.
- **Docker Compose**: For containerized and reproducible development setup.

## ğŸ“‚ Directory Structure

```
.
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ elt_dag.py               # Contains main DAG and trigger DAG
â”œâ”€â”€ customer_postgres/              # dbt project directory
â”œâ”€â”€ elt_script/
â”‚   â””â”€â”€ elt_script.py               # Python script for data extraction and loading
â”œâ”€â”€ source_db_init/
â”‚   â””â”€â”€ init.sql                    # Sample data for the source database
â”œâ”€â”€ Dockerfile                      # Custom Airflow image with providers installed
â”œâ”€â”€ docker-compose.yml              # Services configuration
â””â”€â”€ README.md
```

## ğŸš€ Pipeline Overview

1. **Trigger DAG** (`trigger_elt_once`)

   - Runs once on startup and triggers the main ELT workflow.

2. **Main DAG** (`elt_and_dbt`)
   - **`run_elt_script`**: PythonOperator that extracts data from source PostgreSQL and loads it into destination PostgreSQL.
   - **`dbt_run`**: DockerOperator that runs `dbt run` inside a container to transform the data.

## ğŸ³ Getting Started

1. Start the Project

```bash
docker compose up --build
```

2. Access Airflow UI
   â€¢ URL: http://localhost:8080
   â€¢ Login: airflow
   â€¢ Password: airflow

## ğŸ“š References

This project learns from the GitHub repository: justinbchau/custom-elt-project
# dbt-airflow-exercise
